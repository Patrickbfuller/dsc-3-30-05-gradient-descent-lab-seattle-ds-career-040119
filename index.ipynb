{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent - Lab\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this lab, we'll continue to formalize our work with gradient descent and once again practice coding some implementations, starting with a review of linear regression. In the upcoming labs, you'll apply similar procedures to implement logistic regression on your own.\n",
    "\n",
    "## Objectives\n",
    "You will be able to:\n",
    "* Create a full gradient descent algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using Gradient Descent to Minimize OLS\n",
    "\n",
    "In order to practice gradient descent, lets begin by investigating a simple regression case in which we are looking to minimize the Residual Sum of Squares (RSS) between our predictions and the actual values. Remember that this is referred to Ordinary Least Squares (OLS) regression. Below, is a mock dataset that we will work with. Preview the dataset. Then, we will compare to simplistic models. Finally, we will use gradient descent to improve upon these  initial models.\n",
    "\n",
    "Good luck!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>budget</th>\n",
       "      <th>domgross</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>13000000</td>\n",
       "      <td>25682380</td>\n",
       "      <td>21 &amp;amp; Over</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45658735</td>\n",
       "      <td>13414714</td>\n",
       "      <td>Dredd 3D</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>20000000</td>\n",
       "      <td>53107035</td>\n",
       "      <td>12 Years a Slave</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>61000000</td>\n",
       "      <td>75612460</td>\n",
       "      <td>2 Guns</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>40000000</td>\n",
       "      <td>95020213</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     budget  domgross             title\n",
       "0  13000000  25682380     21 &amp; Over\n",
       "1  45658735  13414714          Dredd 3D\n",
       "2  20000000  53107035  12 Years a Slave\n",
       "3  61000000  75612460            2 Guns\n",
       "4  40000000  95020213                42"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#The dataset\n",
    "import pandas as pd\n",
    "df = pd.read_excel('movie_data.xlsx')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Two Simplistic Models\n",
    "\n",
    "Let's imagine someone is attempting to predict the domestic gross sales of a movie based on the movie's budget, or at least further investigate how these two quantities are related. Two models are suggested, and need to be compared.  \n",
    "The two models are:  \n",
    "$domgross = 1.575 \\bullet budget$  \n",
    "$domgross = 1.331 \\bullet budget$  \n",
    "Here's a graph of the two models along with the actual data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "x = np.linspace(start=df.budget.min(), stop=df.budget.max(), num=10**5)\n",
    "plt.scatter(x, 1.575*x, label='Mean Ratio Model') #Model 1\n",
    "plt.scatter(x, 1.331*x, label='Median Ratio Model') #Model 2\n",
    "plt.scatter(df.budget, df.domgross, label='Actual Data Points')\n",
    "plt.title('Gross Domestic Sales vs. Budget', fontsize=20)\n",
    "plt.xlabel('Budget', fontsize=16)\n",
    "plt.ylabel('Gross Domestic Sales', fontsize=16)\n",
    "plt.legend(bbox_to_anchor=(1,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Error/Loss Functions\n",
    "\n",
    "In compare the two models (and future ones), we need to define a metric for evaluating and comparing models to each other. Traditionally this is the residual sum of squares. As such we are looking to minimize  $ \\sum(\\hat{y}-y)^2$.\n",
    "Write a function **rss(m)** which calculates the residual sum of squares for a simplistic model $domgross = m \\bullet budget$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rss(m, X=df.budget, y=df.domgross):\n",
    "    #Your code here\n",
    "    return ((y-m*X)**2).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run your RSS function on the two models\n",
    "Which of the two models is better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.7614512142376128e+17"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your code here\n",
    "rss(1.575)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.3547212057814554e+17"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Your response here\n",
    "rss(1.331)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Descent\n",
    "\n",
    "Now that we have a loss function, we can use numerical methods to find a minimum to the loss function. By minimizing our loss, we have achieved an optimal solution according to our problem formulation. Here's our outline of gradient descent from the previous lesson:  \n",
    "\n",
    "1. Define initial parameters:\n",
    "    1. pick a starting point\n",
    "    2. pick a step size $\\alpha$ (alpha)\n",
    "    3. choose a maximum number of iterations; the algorithm will terminate after this many iterations if a minimum has yet to be found\n",
    "    4. (optionally) define a precision parameter; similar to the maximum number of iterations, this will terminate the algorithm early. For example, one might define a precision parameter of 0.00001, in which case if the change in the loss function were less then 0.00001, the algorithm would terminate. The idea is that we are very close to the bottom and further iterations would make a negligable difference.\n",
    "2. Calculate the gradient at the current point (initially, the starting point)\n",
    "3. Take a step (of size alpha) in the direction of the gradient\n",
    "4. Repeat steps 2 and 3 until the maximum number of iterations is met, or the difference between two points is less then your precision parameter  \n",
    "\n",
    "To start, lets simply visualize our cost function. Plot the cost function output for a range of m values from -3 to 5."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Your code here\n",
    "for m in range(-3,5):\n",
    "    plt.scatter(m,rss(m), s=rss(m)/(1e16), marker=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, this is a simple cost function. The minimum is clearly around 1. With that, let's try and implement gradient descent in order to find our optimal value for m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cur x: 1.5, rss: 2.6084668957174013e+17\n",
      "Cur x: 1.133065571442482, rss: 2.2177730533770314e+17\n",
      "Cur x: 1.1131830522748978, rss: 2.2135715390729418e+17\n",
      "Cur x: 1.1124754156940848, rss: 2.21345414998669e+17\n",
      "Cur x: 1.1124506992634624, rss: 2.2134500897406422e+17\n",
      "Cur x: 1.1124498365366489, rss: 2.213449948066475e+17\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('The local minimum occurs at', 1.1124498365366489)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cur_x = 1.5 #Set a starting point\n",
    "alpha = 1e-7 #Initialize a step size\n",
    "precision = 0.0000001 #Initialize a precision\n",
    "previous_step_size = 1 #Helpful initialization\n",
    "max_iters = 10000 # maximum number of iterations\n",
    "iters = 0 #iteration counter\n",
    "\n",
    "#Create a loop to iterate through the algorithm until either the max_iteration or precision conditions is met\n",
    "#Your code here; create a loop as described above\n",
    "while (previous_step_size > precision) & (iters < max_iters):\n",
    "    prev_x = cur_x\n",
    "    print(f'Cur x: {cur_x}, rss: {rss(cur_x)}')\n",
    "    #Calculate the gradient. This is often done by hand to reduce computational complexity.\n",
    "    #For here, generate points surrounding your current state, then calculate the rss of these points\n",
    "    #Finally, use the np.gradient() method on this survey region. \n",
    "    #This code is provided here to ease this portion of the algorithm implementation\n",
    "    x_survey_region = np.linspace(start = cur_x - previous_step_size ,\n",
    "                                  stop = cur_x + previous_step_size , num = 101)\n",
    "    rss_survey_region = [np.sqrt(rss(m)) for m in x_survey_region]\n",
    "    gradient = np.gradient(rss_survey_region)[50] \n",
    "    \n",
    "    #Update the current x, by taking a \"alpha sized\" step in the direction of the gradient\n",
    "    cur_x -= alpha * gradient\n",
    "    previous_step_size = np.abs(cur_x - prev_x)\n",
    "    \n",
    "    #Update the iteration number\n",
    "    iters+=1\n",
    "#The output for the above will be: ('The local minimum occurs at', 1.1124498053361267)\n",
    "\"The local minimum occurs at\", prev_x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot the minimum on your graph\n",
    "Replot the RSS cost curve as above. Add a red dot for the minimum of this graph using the solution from your gradient descent function above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/learn-env/lib/python3.6/site-packages/ipykernel_launcher.py:3: DeprecationWarning: object of type <class 'float'> cannot be safely interpreted as an integer.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "#Your code here\n",
    "plt.figure()\n",
    "x = np.linspace(-3,5,1e5)\n",
    "y = [rss(xi) for xi in x]\n",
    "plt.plasma()\n",
    "plt.plot(x,y,c=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary \n",
    "\n",
    "In this lab you coded up a gradient descent algorithm from scratch! In the next lab, you'll apply this to logistic regression in order to create a full implementation yourself!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
